{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ariel Mashraki', 'Mattias Wadman', 'Henrik Rydgård', 'Mark Tyneway', 'Lincoln Stein']\n"
     ]
    }
   ],
   "source": [
    "x = soup.find_all('h1',class_='h3 lh-condensed')\n",
    "tren = [i.text.strip() for i in x]\n",
    "print(tren[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ariel Mashraki (a8m) ', 'Mattias Wadman (wader) ', 'Henrik Rydgård (hrydgard) ', 'ppsspp (https://discord.gg/5NJB6dD) ', 'Mark Tyneway (tynes) ']\n",
      "\n",
      "[['golang-cheat-sheet'], ['fq'], [], ['Net-ISP-Balance'], ['nas-tools']]\n"
     ]
    }
   ],
   "source": [
    "x = soup.find_all('div',class_='col-md-6')\n",
    "all = []\n",
    "\n",
    "for i in x:\n",
    "    p=0\n",
    "    nick = []\n",
    "    for j in i.find_all('a'):\n",
    "        nick.append(j.text.strip())\n",
    "    all.append(nick)\n",
    "\n",
    "devs = []\n",
    "repos = []\n",
    "  \n",
    "for i in all:\n",
    "    if len(i) == 2:\n",
    "        devs.append(i[0] + ' ('+ i[1]+') ')\n",
    "    else:\n",
    "        repos.append(i)\n",
    "        \n",
    "print(devs[:5])\n",
    "print()\n",
    "print(repos[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatGPT', 'chatGPT-discord-bot', 'Python', 'Open-Assistant', 'Lsmith']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = soup.find_all('h1',class_= 'h3 lh-condensed')\n",
    "lst = []\n",
    "for i in x:\n",
    "    y = i.find('a').text.strip().split('\\n')[-1].strip()\n",
    "    lst.append(y)\n",
    "    \n",
    "lst[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "url2 = 'https://en.wikipedia.org'\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = soup.select('div img')\n",
    "\n",
    "b = url2 + y[1]['src']\n",
    "    \n",
    "Image(url = b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = soup.find_all('a', class_ = 'image')[0].find('img')\n",
    "\n",
    "Image(url = x['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (719059680.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_14884\\719059680.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    #print(i.find('img')['src'])\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('a', class_ = 'image'):\n",
    "    #print(i.find('img')['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    if link.has_attr('href'):\n",
    "        links.append(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "lis = []\n",
    "for i in soup.find_all('div', class_='usctitlechanged'):\n",
    "    lis.append(i.text.strip())\n",
    "len(lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RUJA IGNATOVA',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'ALEXIS FLORES',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'MICHAEL JAMES PRATT',\n",
       " 'RAFAEL CARO-QUINTERO']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = []\n",
    "for i in soup.find_all('h3', class_='title'):\n",
    "    top.append(i.find('a').text.strip())\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   17:24:14.410min ago</td>\n",
       "      <td>39.01</td>\n",
       "      <td>N</td>\n",
       "      <td>40.27</td>\n",
       "      <td>E</td>\n",
       "      <td>7</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.5</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2023-02-07 17:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   17:22:07.012min ago</td>\n",
       "      <td>38.13</td>\n",
       "      <td>N</td>\n",
       "      <td>37.06</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 17:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   17:17:12.317min ago</td>\n",
       "      <td>38.00</td>\n",
       "      <td>N</td>\n",
       "      <td>38.44</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.5</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2023-02-07 17:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   17:08:29.026min ago</td>\n",
       "      <td>10.71</td>\n",
       "      <td>N</td>\n",
       "      <td>84.96</td>\n",
       "      <td>W</td>\n",
       "      <td>135</td>\n",
       "      <td>M</td>\n",
       "      <td>3.2</td>\n",
       "      <td>COSTA RICA</td>\n",
       "      <td>2023-02-07 17:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   17:03:50.530min ago</td>\n",
       "      <td>38.11</td>\n",
       "      <td>N</td>\n",
       "      <td>36.66</td>\n",
       "      <td>E</td>\n",
       "      <td>19</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.1</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 17:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   17:02:49.032min ago</td>\n",
       "      <td>19.20</td>\n",
       "      <td>N</td>\n",
       "      <td>155.42</td>\n",
       "      <td>W</td>\n",
       "      <td>33</td>\n",
       "      <td>Md</td>\n",
       "      <td>2.1</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2023-02-07 17:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>V</td>\n",
       "      <td>earthquake2023-02-07   16:54:30.040min ago</td>\n",
       "      <td>38.02</td>\n",
       "      <td>N</td>\n",
       "      <td>37.51</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.6</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 17:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:50:12.644min ago</td>\n",
       "      <td>47.76</td>\n",
       "      <td>N</td>\n",
       "      <td>122.82</td>\n",
       "      <td>W</td>\n",
       "      <td>14</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PUGET SOUND REGION, WASHINGTON</td>\n",
       "      <td>2023-02-07 17:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>III</td>\n",
       "      <td>earthquake2023-02-07   16:49:13.445min ago</td>\n",
       "      <td>38.07</td>\n",
       "      <td>N</td>\n",
       "      <td>36.54</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.8</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:47:01.747min ago</td>\n",
       "      <td>38.01</td>\n",
       "      <td>N</td>\n",
       "      <td>36.51</td>\n",
       "      <td>E</td>\n",
       "      <td>15</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 17:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   16:44:34.550min ago</td>\n",
       "      <td>9.75</td>\n",
       "      <td>N</td>\n",
       "      <td>138.39</td>\n",
       "      <td>E</td>\n",
       "      <td>20</td>\n",
       "      <td>mb</td>\n",
       "      <td>5.2</td>\n",
       "      <td>STATE OF YAP, MICRONESIA</td>\n",
       "      <td>2023-02-07 17:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   16:43:25.651min ago</td>\n",
       "      <td>38.09</td>\n",
       "      <td>N</td>\n",
       "      <td>38.52</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.9</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2023-02-07 16:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:42:25.052min ago</td>\n",
       "      <td>2.42</td>\n",
       "      <td>S</td>\n",
       "      <td>140.68</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>3.3</td>\n",
       "      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n",
       "      <td>2023-02-07 16:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:38:04.756min ago</td>\n",
       "      <td>32.09</td>\n",
       "      <td>N</td>\n",
       "      <td>116.42</td>\n",
       "      <td>W</td>\n",
       "      <td>10</td>\n",
       "      <td>Ml</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BAJA CALIFORNIA, MEXICO</td>\n",
       "      <td>2023-02-07 16:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>V</td>\n",
       "      <td>earthquake2023-02-07   16:36:45.758min ago</td>\n",
       "      <td>37.35</td>\n",
       "      <td>N</td>\n",
       "      <td>37.07</td>\n",
       "      <td>E</td>\n",
       "      <td>7</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.4</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   16:33:40.01hr 01min ago</td>\n",
       "      <td>37.85</td>\n",
       "      <td>N</td>\n",
       "      <td>37.82</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:30:21.91hr 04min ago</td>\n",
       "      <td>39.63</td>\n",
       "      <td>N</td>\n",
       "      <td>25.87</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.3</td>\n",
       "      <td>AEGEAN SEA</td>\n",
       "      <td>2023-02-07 16:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   16:25:49.71hr 09min ago</td>\n",
       "      <td>38.05</td>\n",
       "      <td>N</td>\n",
       "      <td>38.47</td>\n",
       "      <td>E</td>\n",
       "      <td>9</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.3</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2023-02-07 16:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>IV</td>\n",
       "      <td>earthquake2023-02-07   16:24:25.81hr 10min ago</td>\n",
       "      <td>37.57</td>\n",
       "      <td>N</td>\n",
       "      <td>37.35</td>\n",
       "      <td>E</td>\n",
       "      <td>4</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.3</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:19:21.11hr 15min ago</td>\n",
       "      <td>38.18</td>\n",
       "      <td>N</td>\n",
       "      <td>37.96</td>\n",
       "      <td>E</td>\n",
       "      <td>6</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:13:49.51hr 21min ago</td>\n",
       "      <td>35.34</td>\n",
       "      <td>S</td>\n",
       "      <td>177.23</td>\n",
       "      <td>E</td>\n",
       "      <td>314</td>\n",
       "      <td>M</td>\n",
       "      <td>3.1</td>\n",
       "      <td>OFF E. COAST OF N. ISLAND, N.Z.</td>\n",
       "      <td>2023-02-07 16:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   16:12:27.71hr 22min ago</td>\n",
       "      <td>38.13</td>\n",
       "      <td>N</td>\n",
       "      <td>36.57</td>\n",
       "      <td>E</td>\n",
       "      <td>8</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:11:18.51hr 23min ago</td>\n",
       "      <td>21.13</td>\n",
       "      <td>S</td>\n",
       "      <td>178.96</td>\n",
       "      <td>W</td>\n",
       "      <td>639</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.8</td>\n",
       "      <td>FIJI REGION</td>\n",
       "      <td>2023-02-07 17:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:09:58.01hr 24min ago</td>\n",
       "      <td>15.66</td>\n",
       "      <td>N</td>\n",
       "      <td>95.46</td>\n",
       "      <td>W</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>4.1</td>\n",
       "      <td>OFFSHORE OAXACA, MEXICO</td>\n",
       "      <td>2023-02-07 17:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>IV</td>\n",
       "      <td>earthquake2023-02-07   16:07:36.71hr 27min ago</td>\n",
       "      <td>38.00</td>\n",
       "      <td>N</td>\n",
       "      <td>37.61</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.6</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   16:02:58.61hr 31min ago</td>\n",
       "      <td>35.77</td>\n",
       "      <td>N</td>\n",
       "      <td>25.50</td>\n",
       "      <td>E</td>\n",
       "      <td>14</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CRETE, GREECE</td>\n",
       "      <td>2023-02-07 16:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   16:00:39.31hr 34min ago</td>\n",
       "      <td>32.00</td>\n",
       "      <td>N</td>\n",
       "      <td>116.82</td>\n",
       "      <td>W</td>\n",
       "      <td>12</td>\n",
       "      <td>Ml</td>\n",
       "      <td>2.3</td>\n",
       "      <td>BAJA CALIFORNIA, MEXICO</td>\n",
       "      <td>2023-02-07 16:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>VI</td>\n",
       "      <td>earthquake2023-02-07   15:59:48.91hr 35min ago</td>\n",
       "      <td>38.06</td>\n",
       "      <td>N</td>\n",
       "      <td>37.38</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.8</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:53:11.01hr 41min ago</td>\n",
       "      <td>5.28</td>\n",
       "      <td>N</td>\n",
       "      <td>95.52</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>2.8</td>\n",
       "      <td>NORTHERN SUMATRA, INDONESIA</td>\n",
       "      <td>2023-02-07 16:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>63</td>\n",
       "      <td></td>\n",
       "      <td>IV</td>\n",
       "      <td>earthquake2023-02-07   15:48:54.01hr 45min ago</td>\n",
       "      <td>37.99</td>\n",
       "      <td>N</td>\n",
       "      <td>36.46</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>mb</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:42:09.51hr 52min ago</td>\n",
       "      <td>38.25</td>\n",
       "      <td>N</td>\n",
       "      <td>38.66</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.8</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2023-02-07 15:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:39:35.01hr 55min ago</td>\n",
       "      <td>2.44</td>\n",
       "      <td>S</td>\n",
       "      <td>140.68</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n",
       "      <td>2023-02-07 16:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:37:52.71hr 56min ago</td>\n",
       "      <td>38.03</td>\n",
       "      <td>N</td>\n",
       "      <td>37.72</td>\n",
       "      <td>E</td>\n",
       "      <td>4</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.3</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:32:31.32hr 02min ago</td>\n",
       "      <td>37.35</td>\n",
       "      <td>N</td>\n",
       "      <td>8.55</td>\n",
       "      <td>W</td>\n",
       "      <td>9</td>\n",
       "      <td>ML</td>\n",
       "      <td>1.9</td>\n",
       "      <td>PORTUGAL</td>\n",
       "      <td>2023-02-07 16:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>271</td>\n",
       "      <td>1</td>\n",
       "      <td>IV</td>\n",
       "      <td>earthquake2023-02-07   15:31:29.32hr 03min ago</td>\n",
       "      <td>37.36</td>\n",
       "      <td>N</td>\n",
       "      <td>37.08</td>\n",
       "      <td>E</td>\n",
       "      <td>4</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.9</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 15:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:28:45.02hr 06min ago</td>\n",
       "      <td>2.42</td>\n",
       "      <td>S</td>\n",
       "      <td>140.68</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>3.3</td>\n",
       "      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n",
       "      <td>2023-02-07 15:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>IV</td>\n",
       "      <td>earthquake2023-02-07   15:26:09.92hr 08min ago</td>\n",
       "      <td>38.04</td>\n",
       "      <td>N</td>\n",
       "      <td>37.05</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.2</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 15:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   15:23:50.72hr 10min ago</td>\n",
       "      <td>38.52</td>\n",
       "      <td>N</td>\n",
       "      <td>40.42</td>\n",
       "      <td>E</td>\n",
       "      <td>8</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.9</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2023-02-07 16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   15:21:54.92hr 12min ago</td>\n",
       "      <td>37.82</td>\n",
       "      <td>N</td>\n",
       "      <td>36.41</td>\n",
       "      <td>E</td>\n",
       "      <td>8</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.9</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 15:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:12:50.72hr 21min ago</td>\n",
       "      <td>38.01</td>\n",
       "      <td>N</td>\n",
       "      <td>37.68</td>\n",
       "      <td>E</td>\n",
       "      <td>6</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 15:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   15:06:53.22hr 27min ago</td>\n",
       "      <td>38.09</td>\n",
       "      <td>N</td>\n",
       "      <td>37.10</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.3</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 15:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:02:20.02hr 32min ago</td>\n",
       "      <td>2.43</td>\n",
       "      <td>S</td>\n",
       "      <td>140.68</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>3.6</td>\n",
       "      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n",
       "      <td>2023-02-07 15:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   15:00:35.02hr 34min ago</td>\n",
       "      <td>38.02</td>\n",
       "      <td>N</td>\n",
       "      <td>36.52</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 15:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   14:51:34.42hr 43min ago</td>\n",
       "      <td>37.40</td>\n",
       "      <td>N</td>\n",
       "      <td>36.96</td>\n",
       "      <td>E</td>\n",
       "      <td>6</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 15:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   14:47:40.62hr 47min ago</td>\n",
       "      <td>38.08</td>\n",
       "      <td>N</td>\n",
       "      <td>37.23</td>\n",
       "      <td>E</td>\n",
       "      <td>28</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.5</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 14:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   14:44:31.32hr 50min ago</td>\n",
       "      <td>38.02</td>\n",
       "      <td>N</td>\n",
       "      <td>38.31</td>\n",
       "      <td>E</td>\n",
       "      <td>3</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.1</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2023-02-07 14:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   14:39:50.92hr 54min ago</td>\n",
       "      <td>38.00</td>\n",
       "      <td>N</td>\n",
       "      <td>36.40</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 14:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2023-02-07   14:37:20.92hr 57min ago</td>\n",
       "      <td>28.19</td>\n",
       "      <td>N</td>\n",
       "      <td>16.27</td>\n",
       "      <td>W</td>\n",
       "      <td>20</td>\n",
       "      <td>ML</td>\n",
       "      <td>1.8</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "      <td>2023-02-07 14:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2023-02-07   14:24:42.43hr 10min ago</td>\n",
       "      <td>38.15</td>\n",
       "      <td>N</td>\n",
       "      <td>36.90</td>\n",
       "      <td>E</td>\n",
       "      <td>8</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.4</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 14:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>V</td>\n",
       "      <td>earthquake2023-02-07   14:23:16.93hr 11min ago</td>\n",
       "      <td>37.98</td>\n",
       "      <td>N</td>\n",
       "      <td>36.49</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>4.1</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-02-07 16:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1    2                                               3      4  5   \\\n",
       "0             F      earthquake2023-02-07   17:24:14.410min ago  39.01  N   \n",
       "1                    earthquake2023-02-07   17:22:07.012min ago  38.13  N   \n",
       "2             F      earthquake2023-02-07   17:17:12.317min ago  38.00  N   \n",
       "3                    earthquake2023-02-07   17:08:29.026min ago  10.71  N   \n",
       "4             F      earthquake2023-02-07   17:03:50.530min ago  38.11  N   \n",
       "5                    earthquake2023-02-07   17:02:49.032min ago  19.20  N   \n",
       "6    20       V      earthquake2023-02-07   16:54:30.040min ago  38.02  N   \n",
       "7                    earthquake2023-02-07   16:50:12.644min ago  47.76  N   \n",
       "8    14     III      earthquake2023-02-07   16:49:13.445min ago  38.07  N   \n",
       "9                    earthquake2023-02-07   16:47:01.747min ago  38.01  N   \n",
       "10    1       F      earthquake2023-02-07   16:44:34.550min ago   9.75  N   \n",
       "11    1       F      earthquake2023-02-07   16:43:25.651min ago  38.09  N   \n",
       "12                   earthquake2023-02-07   16:42:25.052min ago   2.42  S   \n",
       "13                   earthquake2023-02-07   16:38:04.756min ago  32.09  N   \n",
       "14    7       V      earthquake2023-02-07   16:36:45.758min ago  37.35  N   \n",
       "15            F  earthquake2023-02-07   16:33:40.01hr 01min ago  37.85  N   \n",
       "16               earthquake2023-02-07   16:30:21.91hr 04min ago  39.63  N   \n",
       "17            F  earthquake2023-02-07   16:25:49.71hr 09min ago  38.05  N   \n",
       "18    4      IV  earthquake2023-02-07   16:24:25.81hr 10min ago  37.57  N   \n",
       "19               earthquake2023-02-07   16:19:21.11hr 15min ago  38.18  N   \n",
       "20               earthquake2023-02-07   16:13:49.51hr 21min ago  35.34  S   \n",
       "21            F  earthquake2023-02-07   16:12:27.71hr 22min ago  38.13  N   \n",
       "22               earthquake2023-02-07   16:11:18.51hr 23min ago  21.13  S   \n",
       "23               earthquake2023-02-07   16:09:58.01hr 24min ago  15.66  N   \n",
       "24    4      IV  earthquake2023-02-07   16:07:36.71hr 27min ago  38.00  N   \n",
       "25            F  earthquake2023-02-07   16:02:58.61hr 31min ago  35.77  N   \n",
       "26               earthquake2023-02-07   16:00:39.31hr 34min ago  32.00  N   \n",
       "27    6      VI  earthquake2023-02-07   15:59:48.91hr 35min ago  38.06  N   \n",
       "28               earthquake2023-02-07   15:53:11.01hr 41min ago   5.28  N   \n",
       "29   63      IV  earthquake2023-02-07   15:48:54.01hr 45min ago  37.99  N   \n",
       "30               earthquake2023-02-07   15:42:09.51hr 52min ago  38.25  N   \n",
       "31               earthquake2023-02-07   15:39:35.01hr 55min ago   2.44  S   \n",
       "32               earthquake2023-02-07   15:37:52.71hr 56min ago  38.03  N   \n",
       "33               earthquake2023-02-07   15:32:31.32hr 02min ago  37.35  N   \n",
       "34  271  1   IV  earthquake2023-02-07   15:31:29.32hr 03min ago  37.36  N   \n",
       "35               earthquake2023-02-07   15:28:45.02hr 06min ago   2.42  S   \n",
       "36           IV  earthquake2023-02-07   15:26:09.92hr 08min ago  38.04  N   \n",
       "37    1       F  earthquake2023-02-07   15:23:50.72hr 10min ago  38.52  N   \n",
       "38            F  earthquake2023-02-07   15:21:54.92hr 12min ago  37.82  N   \n",
       "39               earthquake2023-02-07   15:12:50.72hr 21min ago  38.01  N   \n",
       "40    2       F  earthquake2023-02-07   15:06:53.22hr 27min ago  38.09  N   \n",
       "41               earthquake2023-02-07   15:02:20.02hr 32min ago   2.43  S   \n",
       "42               earthquake2023-02-07   15:00:35.02hr 34min ago  38.02  N   \n",
       "43    1       F  earthquake2023-02-07   14:51:34.42hr 43min ago  37.40  N   \n",
       "44               earthquake2023-02-07   14:47:40.62hr 47min ago  38.08  N   \n",
       "45               earthquake2023-02-07   14:44:31.32hr 50min ago  38.02  N   \n",
       "46               earthquake2023-02-07   14:39:50.92hr 54min ago  38.00  N   \n",
       "47               earthquake2023-02-07   14:37:20.92hr 57min ago  28.19  N   \n",
       "48            F  earthquake2023-02-07   14:24:42.43hr 10min ago  38.15  N   \n",
       "49    8       V  earthquake2023-02-07   14:23:16.93hr 11min ago  37.98  N   \n",
       "\n",
       "        6  7    8   9    10                                11  \\\n",
       "0    40.27  E    7  ML  2.5                    EASTERN TURKEY   \n",
       "1    37.06  E    5  ML  2.6                    CENTRAL TURKEY   \n",
       "2    38.44  E    0  ML  3.5                    EASTERN TURKEY   \n",
       "3    84.96  W  135   M  3.2                        COSTA RICA   \n",
       "4    36.66  E   19  ML  3.1                    CENTRAL TURKEY   \n",
       "5   155.42  W   33  Md  2.1          ISLAND OF HAWAII, HAWAII   \n",
       "6    37.51  E    5  ML  3.6                    CENTRAL TURKEY   \n",
       "7   122.82  W   14  ML  2.0    PUGET SOUND REGION, WASHINGTON   \n",
       "8    36.54  E    2  ML  3.8                    CENTRAL TURKEY   \n",
       "9    36.51  E   15  ML  3.0                    CENTRAL TURKEY   \n",
       "10  138.39  E   20  mb  5.2          STATE OF YAP, MICRONESIA   \n",
       "11   38.52  E    5  ML  2.9                    EASTERN TURKEY   \n",
       "12  140.68  E   10   M  3.3  NEAR N COAST OF PAPUA, INDONESIA   \n",
       "13  116.42  W   10  Ml  2.0           BAJA CALIFORNIA, MEXICO   \n",
       "14   37.07  E    7  ML  3.4                    CENTRAL TURKEY   \n",
       "15   37.82  E    5  ML  3.0                    CENTRAL TURKEY   \n",
       "16   25.87  E   10  ML  2.3                        AEGEAN SEA   \n",
       "17   38.47  E    9  ML  3.3                    EASTERN TURKEY   \n",
       "18   37.35  E    4  ML  3.3                    CENTRAL TURKEY   \n",
       "19   37.96  E    6  ML  2.7                    CENTRAL TURKEY   \n",
       "20  177.23  E  314   M  3.1   OFF E. COAST OF N. ISLAND, N.Z.   \n",
       "21   36.57  E    8  ML  3.0                    CENTRAL TURKEY   \n",
       "22  178.96  W  639  mb  4.8                       FIJI REGION   \n",
       "23   95.46  W    5   M  4.1           OFFSHORE OAXACA, MEXICO   \n",
       "24   37.61  E    5  ML  3.6                    CENTRAL TURKEY   \n",
       "25   25.50  E   14  ML  3.0                     CRETE, GREECE   \n",
       "26  116.82  W   12  Ml  2.3           BAJA CALIFORNIA, MEXICO   \n",
       "27   37.38  E    2  ML  3.8                    CENTRAL TURKEY   \n",
       "28   95.52  E   10   M  2.8       NORTHERN SUMATRA, INDONESIA   \n",
       "29   36.46  E    1  mb  5.0                    CENTRAL TURKEY   \n",
       "30   38.66  E    5  ML  2.8                    EASTERN TURKEY   \n",
       "31  140.68  E   10   M  3.0  NEAR N COAST OF PAPUA, INDONESIA   \n",
       "32   37.72  E    4  ML  3.3                    CENTRAL TURKEY   \n",
       "33    8.55  W    9  ML  1.9                          PORTUGAL   \n",
       "34   37.08  E    4  mb  4.9                    CENTRAL TURKEY   \n",
       "35  140.68  E   10   M  3.3  NEAR N COAST OF PAPUA, INDONESIA   \n",
       "36   37.05  E    5  ML  3.2                    CENTRAL TURKEY   \n",
       "37   40.42  E    8  ML  3.9                    EASTERN TURKEY   \n",
       "38   36.41  E    8  ML  2.9                    CENTRAL TURKEY   \n",
       "39   37.68  E    6  ML  2.7                    CENTRAL TURKEY   \n",
       "40   37.10  E    5  ML  3.3                    CENTRAL TURKEY   \n",
       "41  140.68  E   10   M  3.6  NEAR N COAST OF PAPUA, INDONESIA   \n",
       "42   36.52  E    5  ML  2.6                    CENTRAL TURKEY   \n",
       "43   36.96  E    6  ML  2.6                    CENTRAL TURKEY   \n",
       "44   37.23  E   28  ML  2.5                    CENTRAL TURKEY   \n",
       "45   38.31  E    3  ML  3.1                    EASTERN TURKEY   \n",
       "46   36.40  E    5  ML  2.7                    CENTRAL TURKEY   \n",
       "47   16.27  W   20  ML  1.8      CANARY ISLANDS, SPAIN REGION   \n",
       "48   36.90  E    8  ML  3.4                    CENTRAL TURKEY   \n",
       "49   36.49  E    5  ML  4.1                    CENTRAL TURKEY   \n",
       "\n",
       "                  12  \n",
       "0   2023-02-07 17:28  \n",
       "1   2023-02-07 17:26  \n",
       "2   2023-02-07 17:23  \n",
       "3   2023-02-07 17:15  \n",
       "4   2023-02-07 17:11  \n",
       "5   2023-02-07 17:06  \n",
       "6   2023-02-07 17:05  \n",
       "7   2023-02-07 17:12  \n",
       "8   2023-02-07 16:59  \n",
       "9   2023-02-07 17:19  \n",
       "10  2023-02-07 17:33  \n",
       "11  2023-02-07 16:55  \n",
       "12  2023-02-07 16:50  \n",
       "13  2023-02-07 16:40  \n",
       "14  2023-02-07 16:40  \n",
       "15  2023-02-07 16:55  \n",
       "16  2023-02-07 16:35  \n",
       "17  2023-02-07 16:36  \n",
       "18  2023-02-07 16:34  \n",
       "19  2023-02-07 16:31  \n",
       "20  2023-02-07 16:20  \n",
       "21  2023-02-07 16:27  \n",
       "22  2023-02-07 17:19  \n",
       "23  2023-02-07 17:05  \n",
       "24  2023-02-07 16:17  \n",
       "25  2023-02-07 16:18  \n",
       "26  2023-02-07 16:04  \n",
       "27  2023-02-07 16:12  \n",
       "28  2023-02-07 16:05  \n",
       "29  2023-02-07 16:02  \n",
       "30  2023-02-07 15:56  \n",
       "31  2023-02-07 16:05  \n",
       "32  2023-02-07 16:10  \n",
       "33  2023-02-07 16:04  \n",
       "34  2023-02-07 15:53  \n",
       "35  2023-02-07 15:35  \n",
       "36  2023-02-07 15:50  \n",
       "37  2023-02-07 16:00  \n",
       "38  2023-02-07 15:48  \n",
       "39  2023-02-07 15:36  \n",
       "40  2023-02-07 15:54  \n",
       "41  2023-02-07 15:10  \n",
       "42  2023-02-07 15:20  \n",
       "43  2023-02-07 15:01  \n",
       "44  2023-02-07 14:58  \n",
       "45  2023-02-07 14:54  \n",
       "46  2023-02-07 14:50  \n",
       "47  2023-02-07 14:45  \n",
       "48  2023-02-07 14:39  \n",
       "49  2023-02-07 16:02  "
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = soup.find_all('table')\n",
    "z= x[3].find_all('th')\n",
    "y = x[3].find('tbody').find_all('tr')\n",
    "row = []\n",
    "rows = []\n",
    "for i in y:\n",
    "    for j in i.find_all('td'):\n",
    "        row.append(j.text.strip())\n",
    "    rows.append(row)\n",
    "    row = []\n",
    "p = pd.DataFrame(rows)\n",
    "cols = []\n",
    "for i in z:\n",
    "    cols.append(i.text.strip())\n",
    "cols\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/arduino'\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "l = []\n",
    "for i in soup.find_all('article'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Free Encyclopedia',\n",
       " 'English',\n",
       " 'Ð\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹',\n",
       " 'æ\\x97¥æ\\x9c¬èª\\x9e',\n",
       " 'Deutsch',\n",
       " 'FranÃ§ais',\n",
       " 'EspaÃ±ol',\n",
       " 'Italiano',\n",
       " 'ä¸\\xadæ\\x96\\x87',\n",
       " 'Ù\\x81Ø§Ø±Ø³Û\\x8c',\n",
       " 'Polski',\n",
       " 'Download Wikipedia for Android or iOS']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "l = []\n",
    "for i in soup.find_all('strong'):\n",
    "    l.append(i.text.strip())\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookies to collect information\n",
      "View cookies\n",
      "change your cookie settings\n",
      "feedback\n",
      "Business and economy\n",
      "Crime and justice\n",
      "Defence\n",
      "Education\n",
      "Environment\n",
      "Government\n",
      "Government spending\n",
      "Health\n",
      "Mapping\n",
      "Society\n",
      "Towns and cities\n",
      "Transport\n",
      "Digital service performance\n",
      "Government reference data\n"
     ]
    }
   ],
   "source": [
    "#your code \n",
    "for i in soup.find_all('a', class_ = 'govuk-link'):\n",
    "    print(i.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "html = req.get(url).text\n",
    "soup = bs(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native speakers(millions)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese(incl. Standard Chinese, but e...</td>\n",
       "      <td>920</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>475</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>373</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi(excl. Urdu)</td>\n",
       "      <td>344</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>234</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Language  \\\n",
       "0  Mandarin Chinese(incl. Standard Chinese, but e...   \n",
       "1                                            Spanish   \n",
       "2                                            English   \n",
       "3                                  Hindi(excl. Urdu)   \n",
       "4                                            Bengali   \n",
       "\n",
       "  Native speakers(millions) Language family      Branch  \n",
       "0                       920    Sino-Tibetan     Sinitic  \n",
       "1                       475   Indo-European     Romance  \n",
       "2                       373   Indo-European    Germanic  \n",
       "3                       344   Indo-European  Indo-Aryan  \n",
       "4                       234   Indo-European  Indo-Aryan  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = soup.find_all('table')[0]\n",
    "x = x.find('tbody')\n",
    "row = []\n",
    "rows = []\n",
    "for i in x.find_all('tr'):\n",
    "    row = []\n",
    "\n",
    "    for j in i.find_all('td'):\n",
    "        row.append(j.text.strip())\n",
    "    rows.append(row)\n",
    "\n",
    "cols = []\n",
    "for i in x.find_all('tr')[0].find_all('th'):\n",
    "    cols.append(i.text.strip())\n",
    "    \n",
    "y = pd.DataFrame(rows[1:], columns=cols)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9412b45e6a57aa9914730508726d49801d3b2c579f461e1fb13c705887a7b1f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
